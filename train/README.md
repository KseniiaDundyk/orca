This directory provides training scripts for Orca models. 

For training Orca models, the minimal required input is a mcool file mapped to hg38 genome with resolutions including 1000, 4000, 32000 (you can use cooler zoomify to generate different resolution). You can optionally supply chromatin annotation tracks, such as CTCF binding sites, DHS sites, and histone marks to simutaneously train the sequence encoding to predict those tracks (note that these tracks are used as auxiliary prediction targets for training the encoder, but are never used as input to the models). You can use other genomes but you will need to supply the reference genome fasta file and change the paths in the scripts to use that file. In addition, haplotypes and unlocalized/unplaced assembly should be removed from the reference genome fasta file.

The mcool files used for training released Orca models can be downloaded from the resource file links we provided. (see README of this repository).

To speed up genome sequence retrieval, the training scripts use a memory-mapped genome file. Run `python misc/make_genome_memmap.py` before you use the training scripts (47Gb space needed). The generated memmap file will also be automatically used by the orce_predict module to speed up other functions too.

The training scripts ending with `_a`, `_b`, `_c` correspond to the first (1Mb), the second(1-32Mb), and the third stage(32-256Mb) of training. In the second and third stage training scripts, the path to the models trained by the previous stage need to be specified.  Note that the training files do not have an exit condition so it will keep on training until you terminate it (the updated model files will be saved periodically). In addition, even though we specified the seed for sampler, the training is not deterministic, model initialization is still random unless you set torch seed (even if you set the seed, there is still nondeterminism in GPU computation that can be turned off at a cost of performance, see https://pytorch.org/docs/stable/notes/randomness.html). For reproducing our training schedule, with a 4x V100 server, the first stage training took 30 days, with the last 10 days trained with `--swa` option with the stochastic weight averaging (total ~480000 steps). The second stage training took 20 days (~150000 steps), and the third stage took 20 days (~36000 steps).
